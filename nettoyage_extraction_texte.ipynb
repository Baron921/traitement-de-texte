{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**TOKENISATION**\n",
    "\n",
    "Processus de rupture d'un flux de contenu textuel : mots, termes, symboles ou autres éléments significatifs"
   ],
   "id": "e0d789092f4a44e4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-01T22:36:41.406219398Z",
     "start_time": "2026-02-01T22:36:40.992126906Z"
    }
   },
   "source": [
    "from os.path import join\n",
    "\n",
    "import nltk, os\n",
    "\n",
    "# Lecture du fichier de base dans une variable brut\n",
    "base_file = open(os.getcwd()+ \"/texte.txt\")\n",
    "text = base_file.read()\n",
    "base_file.close()\n",
    "\n",
    "# Extraction des tokens\n",
    "token_list = nltk.word_tokenize(text)\n",
    "print(\"Liste de  tokens : \", token_list[:20])\n",
    "print(\"Total tokens : \", len(token_list))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste de  tokens :  ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', ',', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', '.', 'Savez-vous', 'pourquoi', 'Spark', 'est', 'le', 'framework']\n",
      "Total tokens :  95\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**NETTOYAGE DU TEXTE**\n",
    "\n",
    "Suppression des ponctuations et conversion en minuscules"
   ],
   "id": "13f3f555212f9907"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T22:36:41.642230776Z",
     "start_time": "2026-02-01T22:36:41.422367068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utilisation de la bibliothèque punkt pour extraire les jetons\n",
    "token_list2 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list))\n",
    "print(\"Liste des tokens après suppression de la ponctuation : \", token_list2[:20])\n",
    "print(\"Total tokens après suppression de la ponctuation : \", len(token_list2))"
   ],
   "id": "11719ffaacfda943",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens après suppression de la ponctuation :  ['Si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'Apache', 'Spark', 'Savez-vous', 'pourquoi', 'Spark', 'est', 'le', 'framework', 'de', 'prédilection']\n",
      "Total tokens après suppression de la ponctuation :  87\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Conversion en minuscule**",
   "id": "efcc08c56bea57e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T22:36:41.738871891Z",
     "start_time": "2026-02-01T22:36:41.665678039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_list3 = [word.lower() for word in token_list2]\n",
    "print(\"Liste des tokens après conversion en minuscule : \", token_list3[:20])\n",
    "print(\"Total tokens après conversion en minuscule : \", len(token_list3))"
   ],
   "id": "78fe7651f7a02112",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens après conversion en minuscule :  ['si', 'vous', 'vous', 'intéressez', 'au', 'big', 'data', 'vous', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'est', 'le', 'framework', 'de', 'prédilection']\n",
      "Total tokens après conversion en minuscule :  87\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Suppression des mots vides**\n",
    "\n",
    "Les mots vides (stopwords) sont des mots très fréquents dans une langue qui portent peu ou pas de sens sémantique dans le contexte d'analyse textuelle.\n",
    "\n",
    "**Caractéristiques des mots vides :**\n",
    "- Fréquence très élevée : \"le\", \"la\", \"les\", \"de\", \"des\", \"un\", \"une\"\n",
    "- Peu de valeur sémantique : \"et\", \"ou\", \"mais\", \"donc\", \"or\", \"ni\", \"car\"\n",
    "\n",
    "**Catégories de mots vides (en français)**\n",
    "- Articles : le, la, les, un, une, des\n",
    "- Pronoms : je, tu, il, elle, nous, vous, ils\n",
    "- Prépositions : à, de, dans, par, pour, sur, avec\n",
    "- Conjonctions : et, ou, mais, donc, car, comme\n",
    "- Auxiliaires : être, avoir, aller\n",
    "- Adverbes courants\t: très, plus, aussi, bien, là\n",
    "\n",
    "Les mots importants (sémantiques) sont conservés. Le sens général est préservé."
   ],
   "id": "db024b66df3508f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Suppression des mots vides\n",
    "token_list4 = list(filter(lambda token: token not in stopwords.words('french'), token_list3))\n",
    "print(\"Liste des tokens après suppression des mots vides : \", token_list4[:20])\n",
    "print(\"Total tokens après suppression des mots vides : \", len(token_list4))"
   ],
   "id": "3c60bac1642fb93a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**STEMMING (RACINISATION)**\n",
    "\n",
    "Le stemming (ou racinisation en français) est une technique qui réduit un mot à sa forme racine en supprimant ses affixes (préfixes, suffixes).\n",
    "\n",
    "Exemple :\n",
    "\n",
    "- marchant → march\n",
    "- marché → march\n",
    "- marchait → march\n",
    "- marcher → march\n",
    "\n",
    "Toutes ces formes renvoient à la même racine : \"march\""
   ],
   "id": "2c90108ab5564ef5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-01T23:28:08.655412711Z",
     "start_time": "2026-02-01T23:28:08.517195775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utilisation de la bibliothèque SnowballStemmer pour la radicalisation\n",
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('french')\n",
    "\n",
    "token_list5 = [stemmer.stem(word) for word in token_list4]\n",
    "\n",
    "print(\"Liste des tokens après le stemming : \", token_list5[:20], \"\\n\")\n",
    "print(\"Total tokens après le stemming : \", len(token_list5))"
   ],
   "id": "2d83157f747e6f81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens après le stemming :  ['si', 'intéress', 'big', 'dat', 'connaiss', 'certain', 'apach', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilect', 'trait', 'don', 'massiv', 'pourquoi', 'est-il', 'aut', 'appréci'] \n",
      "\n",
      "Total tokens après le stemming :  54\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**LEMMATISATION**\n",
    "\n",
    "La lemmatisation est le processus qui transforme un mot en sa forme canonique (le lemme) en tenant compte de son contexte grammatical.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "- étaient → être\n",
    "- suis → être\n",
    "- serai → être\n",
    "\n",
    "Toutes ces formes renvoient au lemme : \"être\""
   ],
   "id": "3ba3294e963bb242"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T00:04:48.401595041Z",
     "start_time": "2026-02-02T00:04:42.446645641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download(\"wordnet\", quiet=True)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list6 = [lemmatizer.lemmatize(word) for word in token_list4]\n",
    "print(\"Liste des tokens après lemmatisation : \", token_list6[:20], \"\\n\")\n",
    "print(\"Total tokens après lemmatisation : \", len(token_list6))"
   ],
   "id": "65fb7c5c70cbe0eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens après lemmatisation :  ['si', 'intéressez', 'big', 'data', 'connaissez', 'certainement', 'apache', 'spark', 'savez-vous', 'pourquoi', 'spark', 'framework', 'prédilection', 'traitement', 'données', 'massif', 'pourquoi', 'est-il', 'autant', 'apprécié'] \n",
      "\n",
      "Total tokens après lemmatisation :  54\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Comparaison STEMMING ET LEMMATISATION**",
   "id": "f18eb1f1adf03026"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T00:08:10.108081537Z",
     "start_time": "2026-02-02T00:08:09.983127128Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Raw : \", token_list4[20],\" , Stemmed : \", token_list5[20],\" , Lemmatized : \", token_list6[20])",
   "id": "1d114352f2f80a46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw :  notamment  , Stemmed :  not  , Lemmatized :  notamment\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **LEMMATISATION EN FRANÇAIS**",
   "id": "7e21fe455278143b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-02T00:25:34.567623061Z",
     "start_time": "2026-02-02T00:25:28.471334391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer\n",
    "from spacy.language import Language\n",
    "\n",
    "# def create_french_lemmatizer(nlp, name):\n",
    "#     lemmatizer = LefffLemmatizer()\n",
    "#     return lemmatizer\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(\" \".join(token_list4))\n",
    "for d in doc:\n",
    "    print(d.text, d.lemma_)"
   ],
   "id": "68e52594fe59db44",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si si\n",
      "intéressez intéresser\n",
      "big big\n",
      "data dater\n",
      "connaissez connaître\n",
      "certainement certainement\n",
      "apache apache\n",
      "spark spark\n",
      "savez savoir\n",
      "-vous vous\n",
      "pourquoi pourquoi\n",
      "spark spark\n",
      "framework framework\n",
      "prédilection prédilection\n",
      "traitement traitemer\n",
      "données donner\n",
      "massives massif\n",
      "pourquoi pourquoi\n",
      "est être\n",
      "-il il\n",
      "autant autant\n",
      "apprécié apprécier\n",
      "notamment notamment\n",
      "déployer déployer\n",
      "algorithmes algorithme\n",
      "machine machine\n",
      "learning learning\n",
      "découvrez découvrir\n",
      "cours cours\n",
      "apache apache\n",
      "pyspark pyspark\n",
      "répondre répondre\n",
      "toutes tout\n",
      "questions question\n",
      "travers traver\n",
      "multiples multiple\n",
      "exemples exemple\n",
      "mises mettre\n",
      "pratique pratique\n",
      "professeur professeur\n",
      "associé associer\n",
      "technologies technologier\n",
      "l' le\n",
      "information information\n",
      "techniques technique\n",
      "d' de\n",
      "optimisation optimisation\n",
      "donne donne\n",
      "toutes tout\n",
      "clés clé\n",
      "analyser analyser\n",
      "efficacement efficacement\n",
      "données donnée\n",
      "grande grand\n",
      "échelle échelle\n",
      "apache apache\n",
      "spark spark\n",
      "python python\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
